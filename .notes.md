### Notes
     A constrastive  vision encoder: takes an image as an input and coverts it into an embedding (a series of embeddings). The embedding will be a vector of a fixed size and will be later cocancatenated with a text embedding 

    
    Contrastive pretraining: when the dot product of the image and text embeddings are a high value the image and text correspond with each other when the dot product is low the two don't correspond

    Constrative learning: Take a series of corresponding texts and images, encode them into embeddings and train models to produce embeddings in a way that the dot product of the image and text is done it should produce a high value 

    The above is made possible through cross-entropy loss. This also takes up a lot of memory since one column or row of embeddings needs to be in memory during the computation also limiting paraellization. 

    Softmax is used during cross-entropy loss to make the computation numerically stable aka make sure that the number produced can fit inside a word

    